{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# チュートリアル\n",
    "このノートブックでは，SageMaker上での機械学習モデルの構築・デプロイを以下の流れで説明していきます．\n",
    "\n",
    "1. ロールの作成\n",
    "2. データセットの準備\n",
    "3. 学習\n",
    "4. デプロイ\n",
    "5. 推論\n",
    "6. エンドポイントの削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ロールの作成\n",
    "\n",
    "AmazonSageMakerFullAccessを持ったIAMロールを取得します．\n",
    "\n",
    "- SageMakerのノートブックインスタンス上の場合\n",
    "    - `role = get_execution_role()` で取得します\n",
    "- オンプレのjupyter notebook上の場合\n",
    "    - AmazonSageMakerFullAccessを許可したIAMロールを発行する\n",
    "    - `role = 'arn:aws:iam::[12桁のAWS ID]:role/[ロール名]'` で取得します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. データセットの準備\n",
    "\n",
    "データセットを準備する一番簡単な方法は，データをs3にアップロードすることです．\n",
    "SageMakerのノートブックインスタンス上では，以下のコマンドで簡単にアップロードすることができます．\n",
    "\n",
    "```\n",
    "sagemaker.Session()\n",
    "s3_dataset_path = sess.upload_data(path=[ディレクトリ/ファイルのパス], key_prefix=[s3でのキー])\n",
    "```\n",
    "\n",
    "ローカルのnotebookでも，権限を持つs3バケットにアップロードし，`s3_dataset_path = 's3://[バケット名]/[キー]'`とすればOKです．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = './dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check datasets.\n",
    "reviews = pd.read_csv(os.path.join(dataset_dir, '10000_review.csv'))\n",
    "sentences = pd.read_csv(os.path.join(dataset_dir, '10000_sentence.csv'))\n",
    "embeddings = np.load(os.path.join(dataset_dir, '10000_embedding.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_title</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RDIJS7QYB6XNR</td>\n",
       "      <td>B00EDBY7X8</td>\n",
       "      <td>Monopoly Junior Board Game</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Excellent!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R36ED1U38IELG8</td>\n",
       "      <td>B00D7JFOPC</td>\n",
       "      <td>56 Pieces of Wooden Train Track Compatible wit...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Good quality track at excellent price</td>\n",
       "      <td>Great quality wooden track (better than some o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R1UE3RPRGCOLD</td>\n",
       "      <td>B002LHA74O</td>\n",
       "      <td>Super Jumbo Playing Cards by S&amp;S Worldwide</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Two Stars</td>\n",
       "      <td>Cards are not as big as pictured.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        review_id  product_id  \\\n",
       "0   RDIJS7QYB6XNR  B00EDBY7X8   \n",
       "1  R36ED1U38IELG8  B00D7JFOPC   \n",
       "2   R1UE3RPRGCOLD  B002LHA74O   \n",
       "\n",
       "                                       product_title  star_rating  \\\n",
       "0                         Monopoly Junior Board Game          5.0   \n",
       "1  56 Pieces of Wooden Train Track Compatible wit...          5.0   \n",
       "2         Super Jumbo Playing Cards by S&S Worldwide          2.0   \n",
       "\n",
       "                         review_headline  \\\n",
       "0                             Five Stars   \n",
       "1  Good quality track at excellent price   \n",
       "2                              Two Stars   \n",
       "\n",
       "                                         review_body  \n",
       "0                                       Excellent!!!  \n",
       "1  Great quality wooden track (better than some o...  \n",
       "2                  Cards are not as big as pictured.  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R36ED1U38IELG8</td>\n",
       "      <td>great quality wooden track better than some ot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R36ED1U38IELG8</td>\n",
       "      <td>perfect match to the various vintages of thoma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R36ED1U38IELG8</td>\n",
       "      <td>there is enough track here to have fun and get...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        review_id                                           sentence\n",
       "0  R36ED1U38IELG8  great quality wooden track better than some ot...\n",
       "1  R36ED1U38IELG8  perfect match to the various vintages of thoma...\n",
       "2  R36ED1U38IELG8  there is enough track here to have fun and get..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data is uploaded to s3://sagemaker-us-west-2-010942746803/data\n"
     ]
    }
   ],
   "source": [
    "# Upload datasets to Amazon S3.\n",
    "sess = sagemaker.Session()\n",
    "s3_dataset_path = sess.upload_data(path=dataset_dir, key_prefix='data')\n",
    "\n",
    "print(f\"Training data is uploaded to {s3_dataset_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 学習\n",
    "学習を行うためには，以下の2つを最低限準備する必要があります．(これら2つは`source_dir`で指定したディレクトリ直下に保存します．)\n",
    "- `entry_point.py`\n",
    "- `requirements.txt`\n",
    "\n",
    "`source_dir`直下に他のファイル等を置いておけば，学習用のインスタンスではカレントディレクトリ直下にコピーされるため．`entry_point.py`内の処理で操作することが可能です．今回の例では，`modules.pickle`を置いておき，`entry_point.py`で操作しています．ただし，**デプロイ先の推論エンドポイントへはコピーされない**ため，推論時も必要なものを置くことはできません．\n",
    "\n",
    "また，独自で作成したモジュールは，`dependencies`にリスト形式で指定してください．これにより，学習/推論時にモジュールを利用することができます．この例では，`search`モジュールを作成・利用しています．\n",
    "\n",
    "`entry_point.py`の詳しい書き方に関しては，解説記事を参照してください．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# Create estimator.\n",
    "estimator = PyTorch(\n",
    "    entry_point='entry_point.py',\n",
    "    source_dir='source_dir',\n",
    "    dependencies=['search'],\n",
    "    role=role,\n",
    "    py_version='py3',\n",
    "    framework_version='1.9.0',\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.m4.xlarge'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-25 10:55:24 Starting - Starting the training job...\n",
      "2020-01-25 10:55:26 Starting - Launching requested ML instances......\n",
      "2020-01-25 10:56:28 Starting - Preparing the instances for training...\n",
      "2020-01-25 10:57:18 Downloading - Downloading input data...\n",
      "2020-01-25 10:57:30 Training - Downloading the training image.....\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-01-25 10:58:29,822 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-01-25 10:58:29,825 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-01-25 10:58:29,839 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2020-01-25 10:58:29,840 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-01-25 10:58:38,597 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-01-25 10:58:38,597 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-01-25 10:58:38,597 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-01-25 10:58:38,598 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m pip install . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /tmp/tmp_44s9yf6/module_dir\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (4.36.1)\u001b[0m\n",
      "\u001b[34mCollecting Unidecode\n",
      "  Downloading Unidecode-1.1.1-py2.py3-none-any.whl (238 kB)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-ml-py\n",
      "  Downloading nvidia-ml-py-375.53.tar.gz (22 kB)\u001b[0m\n",
      "\u001b[34mCollecting clean-text\n",
      "  Downloading clean_text-0.1.1-py3-none-any.whl (9.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting sentence-transformers\n",
      "  Downloading sentence-transformers-0.2.5.tar.gz (49 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 6)) (0.21.2)\u001b[0m\n",
      "\u001b[34mCollecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.6.1-cp36-cp36m-manylinux2010_x86_64.whl (7.1 MB)\u001b[0m\n",
      "\u001b[34mCollecting ftfy\n",
      "  Downloading ftfy-5.6.tar.gz (58 kB)\u001b[0m\n",
      "\u001b[34mCollecting transformers==2.3.0\n",
      "  Downloading transformers-2.3.0-py3-none-any.whl (447 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from sentence-transformers->-r requirements.txt (line 5)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from sentence-transformers->-r requirements.txt (line 5)) (1.16.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /opt/conda/lib/python3.6/site-packages (from sentence-transformers->-r requirements.txt (line 5)) (1.2.2)\u001b[0m\n",
      "\u001b[34mCollecting nltk\n",
      "  Downloading nltk-3.4.5.zip (1.5 MB)\u001b[0m\n",
      "\n",
      "2020-01-25 10:58:29 Training - Training image download completed. Training in progress.\u001b[34mRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.6/site-packages (from scikit-learn->-r requirements.txt (line 6)) (0.14.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wcwidth in /opt/conda/lib/python3.6/site-packages (from ftfy->clean-text->-r requirements.txt (line 4)) (0.1.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers==2.3.0->sentence-transformers->-r requirements.txt (line 5)) (2.22.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: boto3 in /opt/conda/lib/python3.6/site-packages (from transformers==2.3.0->sentence-transformers->-r requirements.txt (line 5)) (1.11.7)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17\n",
      "  Downloading regex-2020.1.8-cp36-cp36m-manylinux2010_x86_64.whl (689 kB)\u001b[0m\n",
      "\u001b[34mCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.38.tar.gz (860 kB)\u001b[0m\n",
      "\u001b[34mCollecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from nltk->sentence-transformers->-r requirements.txt (line 5)) (1.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==2.3.0->sentence-transformers->-r requirements.txt (line 5)) (2019.11.28)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==2.3.0->sentence-transformers->-r requirements.txt (line 5)) (2.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==2.3.0->sentence-transformers->-r requirements.txt (line 5)) (1.25.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==2.3.0->sentence-transformers->-r requirements.txt (line 5)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers==2.3.0->sentence-transformers->-r requirements.txt (line 5)) (0.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers==2.3.0->sentence-transformers->-r requirements.txt (line 5)) (0.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: botocore<1.15.0,>=1.14.7 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers==2.3.0->sentence-transformers->-r requirements.txt (line 5)) (1.14.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==2.3.0->sentence-transformers->-r requirements.txt (line 5)) (7.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.6/site-packages (from botocore<1.15.0,>=1.14.7->boto3->transformers==2.3.0->sentence-transformers->-r requirements.txt (line 5)) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: docutils<0.16,>=0.10 in /opt/conda/lib/python3.6/site-packages (from botocore<1.15.0,>=1.14.7->boto3->transformers==2.3.0->sentence-transformers->-r requirements.txt (line 5)) (0.15.2)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: nvidia-ml-py, sentence-transformers, default-user-module-name, ftfy, nltk, sacremoses\n",
      "  Building wheel for nvidia-ml-py (setup.py): started\n",
      "  Building wheel for nvidia-ml-py (setup.py): finished with status 'done'\n",
      "  Created wheel for nvidia-ml-py: filename=nvidia_ml_py-375.53.1-py3-none-any.whl size=22385 sha256=965070aaf8344785da3d0111b9ed8b54953c5b97f097f66def340d36fd691b96\n",
      "  Stored in directory: /root/.cache/pip/wheels/3e/06/44/34ac3af4f3c0cb0f197db02efafe858509a88ccd28c3cafbe4\n",
      "  Building wheel for sentence-transformers (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for sentence-transformers (setup.py): finished with status 'done'\n",
      "  Created wheel for sentence-transformers: filename=sentence_transformers-0.2.5-py3-none-any.whl size=64944 sha256=9933d0d34876a48c7b68c7a92593444ec758be053d9649fc53a676af6a5f70e8\n",
      "  Stored in directory: /root/.cache/pip/wheels/db/86/e5/5f4959cdb0c5beb4e89f5dd3af4525cf76c3efa0f662ad991c\n",
      "  Building wheel for default-user-module-name (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for default-user-module-name (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34m  Created wheel for default-user-module-name: filename=default_user_module_name-1.0.0-py2.py3-none-any.whl size=406043598 sha256=94abeb8b01f1027ede9f82f1409381addd5ccf545043a80d3f0c206078215d0b\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-hwx3ka50/wheels/ae/b4/17/23a32e1f8dd69e9d1e0e34ad82b5ed542e808782a46e89a2ca\n",
      "  Building wheel for ftfy (setup.py): started\n",
      "  Building wheel for ftfy (setup.py): finished with status 'done'\n",
      "  Created wheel for ftfy: filename=ftfy-5.6-py3-none-any.whl size=44553 sha256=18775fb437946b2982920d49d0909dda9e0dc8c2ebe85f4c7f120f553522c047\n",
      "  Stored in directory: /root/.cache/pip/wheels/22/e3/08/bc85bd99c87c453a4329d3a5f9af5bf1364af38ebd462353f0\n",
      "  Building wheel for nltk (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for nltk (setup.py): finished with status 'done'\n",
      "  Created wheel for nltk: filename=nltk-3.4.5-py3-none-any.whl size=1449906 sha256=eb1c8c993b21145cfba1a2d56b38c0ee44bbd4bdaa58410f4d85322f5df6d56b\n",
      "  Stored in directory: /root/.cache/pip/wheels/e3/c9/b0/ed26a73ef75a53145820825afa8e2d2c9b30fe9f6c10cd3202\n",
      "  Building wheel for sacremoses (setup.py): started\n",
      "  Building wheel for sacremoses (setup.py): finished with status 'done'\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.38-py3-none-any.whl size=884629 sha256=678d2af90cd1415a7b108417e7f45ded9c7d21804a9e9478a48f9200e936d5dc\n",
      "  Stored in directory: /root/.cache/pip/wheels/03/e9/be/8b52f6e7e8c333b56f9440575b4c5eb4d96d27b5d22df5a71e\u001b[0m\n",
      "\u001b[34mSuccessfully built nvidia-ml-py sentence-transformers default-user-module-name ftfy nltk sacremoses\u001b[0m\n",
      "\u001b[34mInstalling collected packages: Unidecode, nvidia-ml-py, ftfy, clean-text, regex, sacremoses, sentencepiece, transformers, nltk, sentence-transformers, faiss-cpu, default-user-module-name\u001b[0m\n",
      "\u001b[34mSuccessfully installed Unidecode-1.1.1 clean-text-0.1.1 default-user-module-name-1.0.0 faiss-cpu-1.6.1 ftfy-5.6 nltk-3.4.5 nvidia-ml-py-375.53.1 regex-2020.1.8 sacremoses-0.0.38 sentence-transformers-0.2.5 sentencepiece-0.1.85 transformers-2.3.0\u001b[0m\n",
      "\u001b[34m2020-01-25 10:59:30,196 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-01-25 10:59:30,213 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-01-25 10:59:30,229 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-01-25 10:59:30,244 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2020-01-25-10-53-51-302\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-010942746803/pytorch-training-2020-01-25-10-53-51-302/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"entry_point\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"entry_point.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=entry_point.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=entry_point\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-010942746803/pytorch-training-2020-01-25-10-53-51-302/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2020-01-25-10-53-51-302\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-010942746803/pytorch-training-2020-01-25-10-53-51-302/source/sourcedir.tar.gz\",\"module_name\":\"entry_point\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"entry_point.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python entry_point.py\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m[nltk_data] Downloading package punkt to /root/nltk_data...\u001b[0m\n",
      "\u001b[34m[nltk_data]   Unzipping tokenizers/punkt.zip.\u001b[0m\n",
      "\u001b[34m2020-01-25 10:59:39,308 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-01-25 10:59:45 Uploading - Uploading generated training model\n",
      "2020-01-25 11:01:07 Completed - Training job completed\n",
      "Training seconds: 229\n",
      "Billable seconds: 229\n"
     ]
    }
   ],
   "source": [
    "# Train.\n",
    "estimator.fit({'train': s3_dataset_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. デプロイ\n",
    "\n",
    "SageMaker APIを用いれば，学習済みモデルを簡単にデプロイすることができます．(少し時間がかかります．)\n",
    "\n",
    "`deploy`関数は`sagemaker.RealTimePredictor`オブジェクトを返し，これを利用して推論することができます．このとき，リクエストのシリアライザとレスポンスのデシリアライザを指定します．指定しない場合，クライアント側でそれらの処理をする必要があります．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------!"
     ]
    }
   ],
   "source": [
    "# Deploy the trained model.\n",
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import JSONSerializer, JSONDeserializer\n",
    "\n",
    "predictor.content_types = 'application/json'\n",
    "predictor.serializer = JSONSerializer()\n",
    "predictor.deserializer = JSONDeserializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = {\n",
    "    'query': 'it still looks brand new too',\n",
    "    'n_items': 1\n",
    "}\n",
    "response = predictor.predict(request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "- review_id\n",
      "R1V5I8W64XQA8R\n",
      "- product_id\n",
      "B00388C3C4\n",
      "- product_title\n",
      "Fisher-Price Laugh & Learn Learning Kitchen Activity Center\n",
      "- star_rating\n",
      "5.0\n",
      "- review_headline\n",
      "One of our most beloved toys, even 1.5 yrs later\n",
      "- review_body\n",
      "My son received this as a gift when he was 9 months old. He played with it daily (sometimes 30 min at a time) until he was almost 2, when he graduated to a big kitchen. This was put away for a few months, but recently came out again now that his little sister is 6 months old. It's one of the few things that they can really play with together, one on each side.  My son (now 2.5) loves playing with it with his sis (and she of course loves it too). We have a lot of toys, but this one stands apart as versatile, fun, and extremely long lived! It still looks brand new too.\n",
      "- product_search_score\n",
      "0.9975305795669556\n",
      "- sentence\n",
      "it still looks brand new too.\n"
     ]
    }
   ],
   "source": [
    "def print_result(response):\n",
    "    for value in response.values():\n",
    "        print('-' * 10)\n",
    "        for k, v in value.items():\n",
    "            print(f'- {k}')\n",
    "            print(f'{v}')\n",
    "\n",
    "print_result(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 推論\n",
    "SageMakerのAPIを利用することで，デプロイしたエンドポイントでの推論を行うことができます．\n",
    "\n",
    "ここでは，データのシリアライズとデシリアライズはクライアント側で行う必要があります．ここではJSON形式でデータの送受信を行なっているので，送信時に`json.dumps`，受信時に`json.load`を行なっています．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = {\n",
    "    'query': 'My children liked it',\n",
    "    'n_items': 1\n",
    "}\n",
    "\n",
    "client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName='pytorch-training-2020-01-25-10-53-51-302', \n",
    "    ContentType='application/json',\n",
    "    Accept='application/json',\n",
    "    Body=json.dumps(request)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "- review_id\n",
      "R2XKMLHEG7Z402\n",
      "- product_id\n",
      "B00IGNWYGQ\n",
      "- product_title\n",
      "Play-Doh Mix 'n Match Magical Designs Palace Set Featuring Disney Princess Aurora\n",
      "- star_rating\n",
      "4.0\n",
      "- review_headline\n",
      "Inventive and fun, some parts hard to do\n",
      "- review_body\n",
      "My kids loved this. Lots of sparkly play doh and tons of molds. One star comes off because it's hard to get play do to press out of the skirt, and once you're done with that it's tough to get the skirt to come off the little pedestal.\n",
      "- product_search_score\n",
      "0.9600517749786377\n",
      "- sentence\n",
      "my kids loved this.\n"
     ]
    }
   ],
   "source": [
    "body = json.load(response['Body'])\n",
    "print_result(body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. エンドポイントの削除\n",
    "エンドポイントは，起動してる時間ずっと課金されてしまうので，こまめに削除します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
